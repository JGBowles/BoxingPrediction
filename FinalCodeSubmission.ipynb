{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing/data clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drawless shape\n",
      "(362655, 26)\n",
      "Clean df shape (81988, 25)\n",
      "[MICE] Completing matrix with shape (81988, 36)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.031\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.919\n",
      "[MICE] Starting imputation round 3/110, elapsed time 1.622\n",
      "[MICE] Starting imputation round 4/110, elapsed time 2.323\n",
      "[MICE] Starting imputation round 5/110, elapsed time 3.031\n",
      "[MICE] Starting imputation round 6/110, elapsed time 3.734\n",
      "[MICE] Starting imputation round 7/110, elapsed time 4.441\n",
      "[MICE] Starting imputation round 8/110, elapsed time 5.141\n",
      "[MICE] Starting imputation round 9/110, elapsed time 5.849\n",
      "[MICE] Starting imputation round 10/110, elapsed time 6.550\n",
      "[MICE] Starting imputation round 11/110, elapsed time 7.222\n",
      "[MICE] Starting imputation round 12/110, elapsed time 7.972\n",
      "[MICE] Starting imputation round 13/110, elapsed time 8.713\n",
      "[MICE] Starting imputation round 14/110, elapsed time 9.493\n",
      "[MICE] Starting imputation round 15/110, elapsed time 10.257\n",
      "[MICE] Starting imputation round 16/110, elapsed time 11.046\n",
      "[MICE] Starting imputation round 17/110, elapsed time 11.824\n",
      "[MICE] Starting imputation round 18/110, elapsed time 12.619\n",
      "[MICE] Starting imputation round 19/110, elapsed time 13.397\n",
      "[MICE] Starting imputation round 20/110, elapsed time 14.176\n",
      "[MICE] Starting imputation round 21/110, elapsed time 14.947\n",
      "[MICE] Starting imputation round 22/110, elapsed time 15.728\n",
      "[MICE] Starting imputation round 23/110, elapsed time 16.509\n",
      "[MICE] Starting imputation round 24/110, elapsed time 17.285\n",
      "[MICE] Starting imputation round 25/110, elapsed time 18.058\n",
      "[MICE] Starting imputation round 26/110, elapsed time 18.830\n",
      "[MICE] Starting imputation round 27/110, elapsed time 19.614\n",
      "[MICE] Starting imputation round 28/110, elapsed time 20.380\n",
      "[MICE] Starting imputation round 29/110, elapsed time 21.149\n",
      "[MICE] Starting imputation round 30/110, elapsed time 21.902\n",
      "[MICE] Starting imputation round 31/110, elapsed time 22.677\n",
      "[MICE] Starting imputation round 32/110, elapsed time 23.540\n",
      "[MICE] Starting imputation round 33/110, elapsed time 24.398\n",
      "[MICE] Starting imputation round 34/110, elapsed time 25.216\n",
      "[MICE] Starting imputation round 35/110, elapsed time 25.981\n",
      "[MICE] Starting imputation round 36/110, elapsed time 26.763\n",
      "[MICE] Starting imputation round 37/110, elapsed time 27.535\n",
      "[MICE] Starting imputation round 38/110, elapsed time 28.307\n",
      "[MICE] Starting imputation round 39/110, elapsed time 29.099\n",
      "[MICE] Starting imputation round 40/110, elapsed time 29.869\n",
      "[MICE] Starting imputation round 41/110, elapsed time 30.639\n",
      "[MICE] Starting imputation round 42/110, elapsed time 31.419\n",
      "[MICE] Starting imputation round 43/110, elapsed time 32.190\n",
      "[MICE] Starting imputation round 44/110, elapsed time 32.968\n",
      "[MICE] Starting imputation round 45/110, elapsed time 33.733\n",
      "[MICE] Starting imputation round 46/110, elapsed time 34.505\n",
      "[MICE] Starting imputation round 47/110, elapsed time 35.280\n",
      "[MICE] Starting imputation round 48/110, elapsed time 36.044\n",
      "[MICE] Starting imputation round 49/110, elapsed time 36.810\n",
      "[MICE] Starting imputation round 50/110, elapsed time 37.573\n",
      "[MICE] Starting imputation round 51/110, elapsed time 38.341\n",
      "[MICE] Starting imputation round 52/110, elapsed time 39.113\n",
      "[MICE] Starting imputation round 53/110, elapsed time 39.873\n",
      "[MICE] Starting imputation round 54/110, elapsed time 40.642\n",
      "[MICE] Starting imputation round 55/110, elapsed time 41.426\n",
      "[MICE] Starting imputation round 56/110, elapsed time 42.202\n",
      "[MICE] Starting imputation round 57/110, elapsed time 42.966\n",
      "[MICE] Starting imputation round 58/110, elapsed time 43.738\n",
      "[MICE] Starting imputation round 59/110, elapsed time 44.499\n",
      "[MICE] Starting imputation round 60/110, elapsed time 45.269\n",
      "[MICE] Starting imputation round 61/110, elapsed time 46.035\n",
      "[MICE] Starting imputation round 62/110, elapsed time 46.799\n",
      "[MICE] Starting imputation round 63/110, elapsed time 47.573\n",
      "[MICE] Starting imputation round 64/110, elapsed time 48.347\n",
      "[MICE] Starting imputation round 65/110, elapsed time 49.110\n",
      "[MICE] Starting imputation round 66/110, elapsed time 49.897\n",
      "[MICE] Starting imputation round 67/110, elapsed time 50.677\n",
      "[MICE] Starting imputation round 68/110, elapsed time 51.440\n",
      "[MICE] Starting imputation round 69/110, elapsed time 52.214\n",
      "[MICE] Starting imputation round 70/110, elapsed time 52.979\n",
      "[MICE] Starting imputation round 71/110, elapsed time 53.740\n",
      "[MICE] Starting imputation round 72/110, elapsed time 54.491\n",
      "[MICE] Starting imputation round 73/110, elapsed time 55.254\n",
      "[MICE] Starting imputation round 74/110, elapsed time 56.015\n",
      "[MICE] Starting imputation round 75/110, elapsed time 56.781\n",
      "[MICE] Starting imputation round 76/110, elapsed time 57.557\n",
      "[MICE] Starting imputation round 77/110, elapsed time 58.322\n",
      "[MICE] Starting imputation round 78/110, elapsed time 59.093\n",
      "[MICE] Starting imputation round 79/110, elapsed time 59.858\n",
      "[MICE] Starting imputation round 80/110, elapsed time 60.624\n",
      "[MICE] Starting imputation round 81/110, elapsed time 61.393\n",
      "[MICE] Starting imputation round 82/110, elapsed time 62.170\n",
      "[MICE] Starting imputation round 83/110, elapsed time 62.948\n",
      "[MICE] Starting imputation round 84/110, elapsed time 63.712\n",
      "[MICE] Starting imputation round 85/110, elapsed time 64.478\n",
      "[MICE] Starting imputation round 86/110, elapsed time 65.243\n",
      "[MICE] Starting imputation round 87/110, elapsed time 66.001\n",
      "[MICE] Starting imputation round 88/110, elapsed time 66.768\n",
      "[MICE] Starting imputation round 89/110, elapsed time 67.532\n",
      "[MICE] Starting imputation round 90/110, elapsed time 68.325\n",
      "[MICE] Starting imputation round 91/110, elapsed time 69.108\n",
      "[MICE] Starting imputation round 92/110, elapsed time 69.900\n",
      "[MICE] Starting imputation round 93/110, elapsed time 70.684\n",
      "[MICE] Starting imputation round 94/110, elapsed time 71.470\n",
      "[MICE] Starting imputation round 95/110, elapsed time 72.244\n",
      "[MICE] Starting imputation round 96/110, elapsed time 72.993\n",
      "[MICE] Starting imputation round 97/110, elapsed time 73.767\n",
      "[MICE] Starting imputation round 98/110, elapsed time 74.543\n",
      "[MICE] Starting imputation round 99/110, elapsed time 75.310\n",
      "[MICE] Starting imputation round 100/110, elapsed time 76.081\n",
      "[MICE] Starting imputation round 101/110, elapsed time 76.860\n",
      "[MICE] Starting imputation round 102/110, elapsed time 77.623\n",
      "[MICE] Starting imputation round 103/110, elapsed time 78.402\n",
      "[MICE] Starting imputation round 104/110, elapsed time 79.177\n",
      "[MICE] Starting imputation round 105/110, elapsed time 79.936\n",
      "[MICE] Starting imputation round 106/110, elapsed time 80.704\n",
      "[MICE] Starting imputation round 107/110, elapsed time 81.483\n",
      "[MICE] Starting imputation round 108/110, elapsed time 82.269\n",
      "[MICE] Starting imputation round 109/110, elapsed time 83.057\n",
      "[MICE] Starting imputation round 110/110, elapsed time 83.847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fancyimpute import KNN, MICE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, make_scorer, f1_score, classification_report\n",
    "\n",
    "df = pd.read_csv(\"bouts_out_new.csv\")\n",
    "\n",
    "# Remove draws\n",
    "df = df[df.result != 'draw']\n",
    "print(\"drawless shape\")\n",
    "print(df.shape)\n",
    "\n",
    "# Random under sample - reduce to 82,000 records (roughly)\n",
    "winAcount, winBcount = df.result.value_counts()\n",
    "df_winA = df[df['result'] == \"win_A\"]\n",
    "df_winB = df[df['result'] == \"win_B\"]\n",
    "df_winA_reduced = df_winA.sample(winBcount)\n",
    "df_winB_reduced = df_winB\n",
    "df = pd.concat([df_winA_reduced, df_winB_reduced], axis=0)\n",
    "\n",
    "# Encode the label \n",
    "le = preprocessing.LabelEncoder().fit(df['result'])\n",
    "encoded = le.transform(df['result'])\n",
    "df['result'] = encoded\n",
    "target = df['result']\n",
    "clean_df = df.drop(['result'], axis=1) #trial \n",
    "print(\"Clean df shape \" + str(clean_df.shape))\n",
    "\n",
    "# Models can only handle numeric features so I convert the non-numeric features - dummies\n",
    "clean_df = pd.get_dummies(clean_df)\n",
    "\n",
    "# Impute with MICE\n",
    "clean_df = pd.DataFrame(MICE().complete(clean_df))\n",
    "\n",
    "\n",
    "# SCALING \n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(clean_df)\n",
    "clean_df = pd.DataFrame(scaled_df, columns=clean_df.columns)\n",
    "\n",
    "# Split the dataset, splits the dataset 90/10%, shuffles the dataset (see the book)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     clean_df, \n",
    "     target, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Select the 20 best features to reduce dimensionality \n",
    "import sklearn.feature_selection\n",
    "selection = sklearn.feature_selection.SelectKBest(chi2, k=20)\n",
    "selected_features = selection.fit(X_train, y_train) \n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [clean_df.columns[i] for i in indices_selected]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7255208079231107\n"
     ]
    }
   ],
   "source": [
    "# Do not need balanced data, purely for testing code\n",
    "test_df = clean_df.sample(2500)\n",
    "test_target = target.sample(2500)\n",
    "\n",
    "x_train_test, x_test_test, y_train_test, y_test_test = train_test_split(\n",
    "     test_df, \n",
    "     test_target, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    2.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.71      0.67      4038\n",
      "          1       0.68      0.62      0.65      4161\n",
      "\n",
      "avg / total       0.66      0.66      0.66      8199\n",
      "\n",
      "0.6623978533967557\n",
      "Refined parameters results - k20: \n",
      "{'C': 773}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def logreg_gridsearch(X, x, Y, y, param_grid):\n",
    "    k = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg_grid = GridSearchCV(estimator = logreg, param_grid=param_grid, \n",
    "                            cv=k, n_jobs=-1, verbose=3)\n",
    "    logreg_grid.fit(X, Y)\n",
    "    prediction = logreg_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(logreg_grid.best_estimator_.score(X, Y)) # \n",
    "    print(logreg_grid.best_estimator_.score(x, y))\n",
    "    return logreg_grid.best_params_\n",
    "\n",
    "\n",
    "# # Do a gridsearch on exponential values of 0.01 to 1000 for both sets of features\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = {'C':Cs}\n",
    "\n",
    "# # # Full range of features \n",
    "print(\"Full range of features best parameters and results: \")\n",
    "full_range_initial = logreg_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(full_range_initial)\n",
    "\n",
    "# # # K20 range of features\n",
    "print(\"K20 range of features best parameters: \")\n",
    "k20_range_initial = logreg_gridsearch(X_train[colnames_selected], y_train, param_grid)\n",
    "print(k20_range_initial)\n",
    "\n",
    "# # Refined the param grid for full range\n",
    "Cs = [x for x in range(50, 151)]\n",
    "param_grid = {'C':Cs}\n",
    "full_range_refine = logreg_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(\"Refined parameters results: \")\n",
    "print(full_range_refine)\n",
    "\n",
    "# # Refined the param grid for K20 range\n",
    "Cs = [x for x in range(750, 1250)]\n",
    "param_grid = {'C':Cs}\n",
    "k20_range_refine = logreg_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(\"Refined parameters results - k20: \")\n",
    "print(k20_range_refine)\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# # Iterate through C levels - full range of features:\n",
    "print(\"Full range of features - growth of C\")\n",
    "for c in Cs:\n",
    "    logreg_clf = LogisticRegression(C=c)\n",
    "    logreg_clf.fit(X_train, y_train)\n",
    "    print(\"When the C is \" + str(c) + \" the score for the training set is \" + str(logreg_clf.score(X_train, y_train)))\n",
    "    print(\"When the C is \" + str(c) + \" the score for the test set is \" + str(logreg_clf.score(X_test, y_test)))\n",
    "\n",
    "print(\"K20 range of features - growth of C\")\n",
    "for c in Cs:\n",
    "    logreg_clf = LogisticRegression(C=c)\n",
    "    logreg_clf.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"When the C is \" + str(c) + \" the score for the training set is \" + str(logreg_clf.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"When the C is \" + str(c) + \" the score for the test set is \" + str(logreg_clf.score(X_test[colnames_selected], y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K20 range of features - growth of K\n",
      "When K is 1 the train score is 0.9651574082857879\n",
      "When K is 1 the test score is 0.6431272106354433\n",
      "When K is 5 the train score is 0.7675669815284121\n",
      "When K is 5 the test score is 0.6515428710818393\n",
      "When K is 10 the train score is 0.7299190936318422\n",
      "When K is 10 the test score is 0.654835955604342\n",
      "When K is 15 the train score is 0.720147989537736\n",
      "When K is 15 the test score is 0.6625198194901817\n",
      "When K is 20 the train score is 0.7123555001422976\n",
      "When K is 20 the test score is 0.6650811074521283\n",
      "When K is 25 the train score is 0.7085202401441949\n",
      "When K is 25 the test score is 0.6702036833760214\n",
      "When K is 30 the train score is 0.702977408556831\n",
      "When K is 30 the test score is 0.6722771069642639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Ks = [x for x in range(1, 31)]\n",
    "param_grid = {'n_neighbors': Ks}\n",
    "\n",
    "\n",
    "def knn_gridsearch(X, x, Y, y, param_grid):\n",
    "    k = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_grid = GridSearchCV(estimator = knn, param_grid=param_grid, \n",
    "                            cv=k, n_jobs=-1, verbose=3)\n",
    "    knn_grid.fit(X, Y)\n",
    "    prediction = knn_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(knn_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(knn_grid.best_estimator_.score(x, y))\n",
    "    return knn_grid.best_params_\n",
    "\n",
    "# Testing 1-30 for full range of features\n",
    "print(\"Full range results:\")\n",
    "knn_initial = knn_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(knn_initial)\n",
    "\n",
    "# Testing 1-30 for K20 range of features\n",
    "print(\"K20 range results:\")\n",
    "knn_initial_k20 = knn_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(knn_initial_k20)\n",
    "\n",
    "Ks = [1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "# Print growth of K and the accuracy on train/test set\n",
    "print(\"Full range of features - growth of K\")\n",
    "for k in Ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"When K is \" + str(k) + \" the train score is \" + str(knn.score(X_train, y_train)))\n",
    "    print(\"When K is \" + str(k) + \" the test score is \" + str(knn.score(X_test, y_test)))\n",
    "    \n",
    "print(\"K20 range of features - growth of K\")\n",
    "for k in Ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"When K is \" + str(k) + \" the train score is \" + str(knn.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"When K is \" + str(k) + \" the test score is \" + str(knn.score(X_test[colnames_selected], y_test)))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When min is 5Training set score: \n",
      "0.8033582241255471\n",
      "When min is 5Ttest set score: \n",
      "0.689840224417612\n",
      "When min is 10Training set score: \n",
      "0.7575654907913103\n",
      "When min is 10Ttest set score: \n",
      "0.6922795462861324\n",
      "When min is 15Training set score: \n",
      "0.7362208459255445\n",
      "When min is 15Ttest set score: \n",
      "0.6904500548847421\n",
      "When min is 20Training set score: \n",
      "0.7289433384379785\n",
      "When min is 20Ttest set score: \n",
      "0.6884985973899256\n",
      "When min is 25Training set score: \n",
      "0.7200937809158546\n",
      "When min is 25Ttest set score: \n",
      "0.6897182583241859\n",
      "When min is 30Training set score: \n",
      "0.7151879006355961\n",
      "When min is 30Ttest set score: \n",
      "0.6871569703622393\n",
      "When min is 35Training set score: \n",
      "0.7114746100367263\n",
      "When min is 35Ttest set score: \n",
      "0.6909379192584462\n",
      "When min is 40Training set score: \n",
      "0.7078019759042675\n",
      "When min is 40Ttest set score: \n",
      "0.685693377241127\n",
      "When min is 45Training set score: \n",
      "0.7062976866470612\n",
      "When min is 45Ttest set score: \n",
      "0.6887425295767776\n",
      "When min is 50Training set score: \n",
      "0.7040751331499275\n",
      "When min is 50Ttest set score: \n",
      "0.685693377241127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rdf_gridsearch(X, x, Y, y, param_grid):\n",
    "    k = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    rdf = RandomForestClassifier()\n",
    "    rdf_grid = GridSearchCV(estimator = rdf, param_grid=param_grid,\n",
    "                            cv=k, n_jobs=-1, verbose=51)\n",
    "    rdf_grid.fit(X, Y)\n",
    "    prediction = rdf_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(rdf_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(rdf_grid.best_estimator_.score(x, y))\n",
    "    return rdf_grid.best_params_\n",
    "\n",
    "n_estimators = [1, 20, 40, 60, 80, 100, 120, 128]\n",
    "max_depth = [1, 10, 20, 30, 40, 50]\n",
    "min_samples_leaf = [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 200]\n",
    "\n",
    "param_grid = {'n_estimators':n_estimators,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "# # Initial test - full range\n",
    "rdf_full_initial = rdf_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(rdf_full_initial)\n",
    "\n",
    "# # K20 range\n",
    "rdf_k20_initial = rdf_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(rdf_k20_initial)\n",
    "\n",
    "# # Refined test - full range\n",
    "n_estimators = [128]\n",
    "max_depth = [57] # edit \n",
    "min_samples_leaf = [x for x in range(1, 100)]  \n",
    "\n",
    "# #Further refined test\n",
    "min_samples_leaf = [x for x in range(1, 100)]  \n",
    "max_depth = [57] # edit \n",
    "param_grid = {'n_estimators':n_estimators,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "rdf_full_refined = rdf_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(rdf_full_refined)\n",
    "\n",
    "# #Refined K20 test\n",
    "n_estimators = [120]\n",
    "min_samples_leaf = [x for x in range(5, 15)]  \n",
    "max_depth = [x for x in range(30, 50)]  \n",
    "param_grid = {'n_estimators':n_estimators,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "rdf_k20_refined = rdf_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(rdf_k20_refined)\n",
    "min_samples_leaf = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "\n",
    "print(\"Full range of features, change in min samples\")\n",
    "for mini in min_samples_leaf:\n",
    "    rdf_full = RandomForestClassifier(min_samples_leaf=mini)\n",
    "    rdf_full.fit(X_train, y_train)\n",
    "    print(\"When min is \" + str(mini) + \"Training set score: \")\n",
    "    print(rdf_full.score(X_train, y_train))\n",
    "    print(\"When min is \" + str(mini) + \"Ttest set score: \")\n",
    "    print(rdf_full.score(X_test, y_test))\n",
    "\n",
    "# # print(\"K20 range of features, change in min samples\")\n",
    "for min in min_samples_leaf:\n",
    "    rdf_k20 = RandomForestClassifier(min_samples_leaf=min)\n",
    "    rdf_k20.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"When min is \" + str(min) + \"Training set score: \")\n",
    "    print(rdf_k20.score(X_train[colnames_selected], y_train))\n",
    "    print(\"When min is \" + str(min) + \"Ttest set score: \")\n",
    "    print(rdf_k20.score(X_test[colnames_selected], y_test))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the alpha is 0.001 the train score is 0.6454891650517015\n",
      "When the alpha is 0.001 the test score is 0.6408098548603488\n",
      "When the alpha is 0.01 the train score is 0.6454891650517015\n",
      "When the alpha is 0.01 the test score is 0.6408098548603488\n",
      "When the alpha is 0.1 the train score is 0.6454891650517015\n",
      "When the alpha is 0.1 the test score is 0.6408098548603488\n",
      "When the alpha is 1 the train score is 0.6454891650517015\n",
      "When the alpha is 1 the test score is 0.6408098548603488\n",
      "When the alpha is 10 the train score is 0.6453671956524685\n",
      "When the alpha is 10 the test score is 0.6399560922063666\n",
      "When the alpha is 100 the train score is 0.6447844529672444\n",
      "When the alpha is 100 the test score is 0.6397121600195146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "def bn_gridsearch(X, x, Y, y, param_grid):\n",
    "    k = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    bn = BernoulliNB()\n",
    "    bn_grid = GridSearchCV(estimator = bn, param_grid=param_grid,\n",
    "                            cv=k, n_jobs=-1, verbose=51)\n",
    "    bn_grid.fit(X, Y)\n",
    "    prediction = bn_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(bn_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(bn_grid.best_estimator_.score(x, y))\n",
    "    return bn_grid.best_params_\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha':alphas}\n",
    "\n",
    "print(\"Initial search - full range\")\n",
    "bn_initial = bn_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(bn_initial)\n",
    "\n",
    "alphas = [x for x in np.linspace(0.0001, 0.1)]\n",
    "param_grid = {'alpha':alphas}\n",
    "\n",
    "print(\"Refined search - full range\")\n",
    "bn_initial = bn_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(bn_initial)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha':alphas}\n",
    "\n",
    "print(\"Initial search - K20 range\")\n",
    "bn_initial = bn_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(bn_initial)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha':alphas}\n",
    "\n",
    "for a in alphas:\n",
    "    bn = BernoulliNB(alpha=a)\n",
    "    bn.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"When the alpha is \" + str(a) + \" the train score is \" + str(bn.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"When the alpha is \" + str(a) + \" the test score is \" + str(bn.score(X_test[colnames_selected], y_test)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full range of features - 2 layer: \n",
      "For layer = (10, 10)train = 0.7634200219544919\n",
      "For layer = (10, 10)test = 0.760946456884986\n",
      "For layer = (25, 25)train = 0.7703180690888886\n",
      "For layer = (25, 25)test = 0.7632638126600805\n",
      "For layer = (50, 50)train = 0.7705349035764139\n",
      "For layer = (50, 50)test = 0.7600926942310038\n",
      "For layer = (75, 75)train = 0.7711311984171082\n",
      "For layer = (75, 75)test = 0.761190389071838\n",
      "For layer = (100, 100)train = 0.7659407228719728\n",
      "For layer = (100, 100)test = 0.7637516770337847\n",
      "For layer = (125, 125)train = 0.7720662971445609\n",
      "For layer = (125, 125)test = 0.7613123551652641\n",
      "For layer = (150, 150)train = 0.7775413679545732\n",
      "For layer = (150, 150)test = 0.7641175753140627\n",
      "For layer = (175, 175)train = 0.7612516770792395\n",
      "For layer = (175, 175)test = 0.7581412367361874\n",
      "For layer = (200, 200)train = 0.7784087059046741\n",
      "For layer = (200, 200)test = 0.7685083546773996\n",
      "K20 range of features - 2 layer: \n",
      "For layer = (10, 10)train = 0.6927048747103227\n",
      "For layer = (10, 10)test = 0.6855714111477009\n",
      "For layer = (25, 25)train = 0.6968518342842429\n",
      "For layer = (25, 25)test = 0.6887425295767776\n",
      "For layer = (50, 50)train = 0.7041022374608682\n",
      "For layer = (50, 50)test = 0.6931333089401146\n",
      "For layer = (75, 75)train = 0.7070972638198105\n",
      "For layer = (75, 75)test = 0.6934992072203927\n",
      "For layer = (100, 100)train = 0.7168141592920354\n",
      "For layer = (100, 100)test = 0.6915477497255763\n",
      "For layer = (125, 125)train = 0.7189011912344658\n",
      "For layer = (125, 125)test = 0.6928893767532626\n",
      "For layer = (150, 150)train = 0.7110680453726165\n",
      "For layer = (150, 150)test = 0.6956945969020613\n",
      "For layer = (175, 175)train = 0.7381723563132716\n",
      "For layer = (175, 175)test = 0.6917916819124283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For layer = (200, 200)train = 0.7523208066242936\n",
      "For layer = (200, 200)test = 0.6850835467739969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools \n",
    "\n",
    "# Do gridsearch on parameters - both sets of features \n",
    "# Print most important parameter change variation - both sets of features \n",
    "\n",
    "def mlp_gridsearch(X, x, Y, y, param_grid):\n",
    "    k = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    mlp = MLPClassifier()\n",
    "    mlp_grid = GridSearchCV(estimator = mlp, param_grid=param_grid, \n",
    "                            cv=k, n_jobs=-1, verbose=51)\n",
    "    mlp_grid.fit(X, Y)\n",
    "    prediction = mlp_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(mlp_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(mlp_grid.best_estimator_.score(x, y))\n",
    "    return mlp_grid.best_params_\n",
    "\n",
    "\n",
    "#hidden_layer_sizes = ([x for x in itertools.product((10, 25, 50, 53, 73, 75, 100, 125, 150, 175, 200), repeat=1)] + \\\n",
    "                      #[x for x in itertools.product((10, 25, 50, 53, 73, 75, 100, 125, 150, 175, 200), repeat=2)] )\n",
    "\n",
    "hidden_layer_sizes = ([x for x in itertools.product((10, 50, 53, 73, 75, 125, 200), repeat=1)] + \\\n",
    "                      [x for x in itertools.product((10, 50, 53, 73, 75, 125, 200), repeat=2)] )\n",
    "    \n",
    "alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "activation = ['relu', 'logistic', 'tanh']  \n",
    "solver=['adam']\n",
    "\n",
    "param_grid = {'hidden_layer_sizes':hidden_layer_sizes,\n",
    "             'alpha': alpha,\n",
    "             'activation':activation,\n",
    "             'solver':solver}\n",
    "\n",
    "#MLP initial search\n",
    "print(\"MLP full range initial search\")\n",
    "full_mlp_initial = mlp_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(full_mlp_initial)\n",
    "\n",
    "K20 initial search \n",
    "print(\"MLP K20 range initial search\")\n",
    "k20_mlp_initial = mlp_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(k20_mlp_initial)\n",
    "\n",
    "Refined search for full range \n",
    "hidden_layer_sizes = ([x for x in itertools.product((x for x in range(190, 211)), repeat=1)])\n",
    "alpha = [0.00001]\n",
    "activation = ['relu', 'logistic', 'tanh'] \n",
    "solver=['adam']\n",
    "\n",
    " param_grid = {'hidden_layer_sizes':hidden_layer_sizes,\n",
    "              'alpha': alpha,\n",
    "              'activation':activation,\n",
    "              'solver':solver}\n",
    "\n",
    "print(\"MLP full range refined search\")\n",
    "full_mlp_refined = mlp_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(full_mlp_refined)\n",
    "\n",
    "# Refined search for K20 range\n",
    "list1 = []\n",
    "for x in range(190, 211):\n",
    "     list1.append(x)\n",
    "\n",
    "print(list1)\n",
    "\n",
    "list2 = []\n",
    "for x in range(5, 16):\n",
    "     list2.append(x)\n",
    "\n",
    "print(list2)\n",
    "\n",
    "hidden_layer_sizes=[]\n",
    "for combo in itertools.product(list2, list1):\n",
    "     hidden_layer_sizes.append(combo)\n",
    "    \n",
    "alpha = [0.00001]\n",
    "activation = ['relu', 'logistic', 'tanh'] \n",
    "solver=['adam']\n",
    "\n",
    "param_grid = {'hidden_layer_sizes':hidden_layer_sizes,\n",
    "              'alpha': alpha,\n",
    "              'activation':activation,\n",
    "              'solver':solver}\n",
    "\n",
    "print(\"MLP K20 range refined search\")\n",
    "k20_mlp_refined = mlp_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(k20_mlp_refined)\n",
    "\n",
    "# 1 layer full range + k20 range - /train and test \n",
    "hidden_layer_sizes = [(10,), (25,), (50,), (75,), (100,), (125,), (150,), (175,), (200,)]\n",
    "alpha = [0.00001]\n",
    "activation = ['relu', 'logistic', 'tanh'] \n",
    "solver=['adam']\n",
    "\n",
    "print(\"Full range of features - 1 layer: \")\n",
    "for layer in hidden_layer_sizes:\n",
    "    mlp_full=MLPClassifier(hidden_layer_sizes=layer)\n",
    "    mlp_full.fit(X_train, y_train)\n",
    "    print(\"For layer = \" + str(layer) + \"train = \" + str(mlp_full.score(X_train, y_train)))\n",
    "    print(\"For layer = \" + str(layer) + \"test = \" + str(mlp_full.score(X_test, y_test)))\n",
    "\n",
    "# # 1 layer full range + k20 range - train and test\n",
    "print(\"K20 range of features - 1 layer: \")\n",
    "for layer in hidden_layer_sizes:\n",
    "    mlp_full=MLPClassifier(hidden_layer_sizes=layer)\n",
    "    mlp_full.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"For layer = \" + str(layer) + \"train = \" + str(mlp_full.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"For layer = \" + str(layer) + \"test = \" + str(mlp_full.score(X_test[colnames_selected], y_test)))\n",
    "    \n",
    "# 2 layer full range + full/k20 range - /train and test \n",
    "hidden_layer_sizes = [(10,10), (25,25), (50,50), (75,75), (100,100), (125,125), (150,150), (175,175), (200,200)]\n",
    "alpha = [0.00001]\n",
    "activation = ['relu', 'logistic', 'tanh']  \n",
    "solver=['adam']\n",
    "\n",
    "print(\"Full range of features - 2 layer: \")\n",
    "for layer in hidden_layer_sizes:\n",
    "    mlp_full=MLPClassifier(hidden_layer_sizes=layer)\n",
    "    mlp_full.fit(X_train, y_train)\n",
    "    print(\"For layer = \" + str(layer) + \"train = \" + str(mlp_full.score(X_train, y_train)))\n",
    "    print(\"For layer = \" + str(layer) + \"test = \" + str(mlp_full.score(X_test, y_test)))\n",
    "\n",
    "# 2 layer full range + k20 range - train and test\n",
    "print(\"K20 range of features - 2 layer: \")\n",
    "for layer in hidden_layer_sizes:\n",
    "    mlp_full=MLPClassifier(hidden_layer_sizes=layer)\n",
    "    mlp_full.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"For layer = \" + str(layer) + \"train = \" + str(mlp_full.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"For layer = \" + str(layer) + \"test = \" + str(mlp_full.score(X_test[colnames_selected], y_test)))\n",
    "    \n",
    "\n",
    "hidden_layer_sizes = ([x for x in itertools.product((10, 25, 50, 53, 73, 75, 100, 125, 200), repeat=1)] + \\\n",
    "                      [x for x in itertools.product((10, 25, 50, 53, 73, 75, 100, 125, 200), repeat=2)] )\n",
    "    \n",
    "alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "activation = ['relu', 'logistic', 'tanh'] \n",
    "solver=['adam']\n",
    "\n",
    "full_mlp_refined = mlp_gridsearch(x_train_test, x_test_test, y_train_test, y_test_test, param_grid)\n",
    "print(full_mlp_refined)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (100,))\n",
    "\n",
    "scores = cross_val_score(mlp, clean_df, target, cv=10, n_jobs=-1)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC # remove l9inear\n",
    "\n",
    "# Do gridsearch on parameters - both sets of features \n",
    "# Retrieve best parameters, refine parameters from selection\n",
    "# Print most important parameter change variation - both sets of features \n",
    "\n",
    "def svc_gridsearch(X, x, Y, y, param_grid):\n",
    "    k=StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    svc = SVC()\n",
    "    svc_grid = GridSearchCV(estimator = svc, param_grid=param_grid, cv=k, n_jobs=-1,\n",
    "                           verbose=51)\n",
    "    svc_grid.fit(x, y)\n",
    "    prediction = svc_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(svc_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(svc_grid.best_estimator_.score(x, y))\n",
    "    return svc_grid.best_params_\n",
    "\n",
    "kernel = ['linear', 'rbf'] \n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "param_grid= {'C': Cs, 'gamma': gammas, 'kernel': kernel}\n",
    "    \n",
    "# Full range of features initial test\n",
    "print(\"Full range of features:\")\n",
    "full_initial_svc = svc_gridsearch(X_train, x_test, Y_train, y_test)\n",
    "\n",
    "# K20 range of features initial test\n",
    "print(\"K20 range of features:\")\n",
    "k20_initial_svc = svc_gridsearch(X_train[colnames_selected}, x_test[colnames_selected], Y_train, y_test)\n",
    "\n",
    "def linearsvc_gridsearch(X, x, Y, y, param_grid):\n",
    "    k=StratifiedKFold(n_splits=10, shuffle=False)\n",
    "    svc = LinearSVC()\n",
    "    svc_grid = GridSearchCV(estimator = svc, param_grid=param_grid, cv=k, n_jobs=-1,\n",
    "                           verbose=51)\n",
    "    svc_grid.fit(x, y)\n",
    "    prediction = svc_grid.predict(x)\n",
    "    print(classification_report(y, prediction))\n",
    "    print(\"Train set: \")\n",
    "    print(svc_grid.best_estimator_.score(X, Y))\n",
    "    print(\"Test set: \")\n",
    "    print(svc_grid.best_estimator_.score(x, y))\n",
    "    return svc_grid.best_params_\n",
    "\n",
    "Cs = [x for x in range(30, 125)]\n",
    "param_grid= {'C': Cs}\n",
    "#Full range of features refined linear test\n",
    "print(\"Full range of features:\")\n",
    "full_refined_svc = linearsvc_gridsearch(X_train, X_test, y_train, y_test, param_grid)\n",
    "print(full_refined_svc)\n",
    "\n",
    "#K20 range of features refined linear test\n",
    "print(\"K20 range of features:\")\n",
    "k20_refined_svc = linearsvc_gridsearch(X_train[colnames_selected], X_test[colnames_selected], y_train, y_test, param_grid)\n",
    "print(k20_refined_svc)\n",
    "\n",
    "gammas = [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "\n",
    "print(\"Analysing gamma change with the RBF kernel - full features\")\n",
    "for g in gammas:\n",
    "    svc = SVC(kernel='rbf', gamma=g)\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(\"When gamma = \" + str(g) + \"the train score = \" + str(svc.score(X_train, y_train)))\n",
    "    print(\"When gamma = \" + str(g) + \"the test score = \" + str(svc.score(X_test, y_test)))\n",
    "\n",
    "print(\"Analysing gamma change with the RBF kernel - K20 features\")\n",
    "for g in gammas:\n",
    "    svc = SVC(kernel='rbf', gamma=g)\n",
    "    svc.fit(X_train[colnames_selected], y_train)\n",
    "    print(\"When gamma = \" + str(g) + \"the train score = \" + str(svc.score(X_train[colnames_selected], y_train)))\n",
    "    print(\"When gamma = \" + str(g) + \"the test score = \" + str(svc.score(X_test[colnames_selected], y_test)))\n",
    "    \n",
    "svc = SVC(kernel='rbf', gamma=1000)\n",
    "svc.fit(X_train[colnames_selected], y_train)\n",
    "print(\"When gamma = \" + str(g) + \"the train score = \" + str(svc.score(X_train[colnames_selected], y_train)))\n",
    "print(\"When gamma = \" + str(g) + \"the test score = \" + str(svc.score(X_test[colnames_selected], y_test)))\n",
    "        \n",
    "                                         \n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
